{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89bea89",
   "metadata": {},
   "source": [
    "So, in this file I was trying to grab information from the website www.dermstore.com for skin-care segment. I've used the scrapy library, manual methods for scrapping main webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5921d945",
   "metadata": {},
   "source": [
    "Here is the libraries for Spider with manual methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b1f6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import requests\n",
    "import sidetable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b30afe",
   "metadata": {},
   "source": [
    "## Let's try the Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f28d878",
   "metadata": {},
   "source": [
    "### Version 1. iterating pages from the all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9903b6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 21:13:05 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: scrapybot)\n",
      "2023-08-10 21:13:05 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.1.1 30 May 2023), cryptography 41.0.1, Platform Windows-10-10.0.19041-SP0\n",
      "2023-08-10 21:13:05 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': 'True',\n",
      " 'AUTOTHROTTLE_TARGET_CONCURRENCY': '1.0',\n",
      " 'HTTPCACHE_ENABLED': 'True',\n",
      " 'LOG_LEVEL': 'INFO'}\n",
      "2023-08-10 21:13:05 [py.warnings] WARNING: C:\\Users\\Dell\\miniconda3\\lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-08-10 21:13:05 [scrapy.extensions.telnet] INFO: Telnet Password: 5d385743cc8a5e2f\n",
      "2023-08-10 21:13:05 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2023-08-10 21:13:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats',\n",
      " 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware']\n",
      "2023-08-10 21:13:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-08-10 21:13:05 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "['__main__.DermStorePipeline']\n",
      "2023-08-10 21:13:05 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-08-10 21:13:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-08-10 21:13:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-08-10 21:14:06 [scrapy.extensions.logstats] INFO: Crawled 96 pages (at 96 pages/min), scraped 442 items (at 442 items/min)\n",
      "2023-08-10 21:15:06 [scrapy.extensions.logstats] INFO: Crawled 225 pages (at 129 pages/min), scraped 1417 items (at 975 items/min)\n",
      "2023-08-10 21:16:05 [scrapy.extensions.logstats] INFO: Crawled 331 pages (at 106 pages/min), scraped 2122 items (at 705 items/min)\n",
      "2023-08-10 21:17:05 [scrapy.extensions.logstats] INFO: Crawled 487 pages (at 156 pages/min), scraped 3205 items (at 1083 items/min)\n",
      "2023-08-10 21:18:06 [scrapy.extensions.logstats] INFO: Crawled 604 pages (at 117 pages/min), scraped 4127 items (at 922 items/min)\n",
      "2023-08-10 21:19:06 [scrapy.extensions.logstats] INFO: Crawled 737 pages (at 133 pages/min), scraped 5049 items (at 922 items/min)\n",
      "2023-08-10 21:20:06 [scrapy.extensions.logstats] INFO: Crawled 859 pages (at 122 pages/min), scraped 5827 items (at 778 items/min)\n",
      "2023-08-10 21:21:05 [scrapy.extensions.logstats] INFO: Crawled 986 pages (at 127 pages/min), scraped 6689 items (at 862 items/min)\n",
      "2023-08-10 21:22:06 [scrapy.extensions.logstats] INFO: Crawled 1128 pages (at 142 pages/min), scraped 7658 items (at 969 items/min)\n",
      "2023-08-10 21:23:06 [scrapy.extensions.logstats] INFO: Crawled 1274 pages (at 146 pages/min), scraped 8546 items (at 888 items/min)\n",
      "2023-08-10 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 1427 pages (at 153 pages/min), scraped 9506 items (at 960 items/min)\n",
      "2023-08-10 21:25:06 [scrapy.extensions.logstats] INFO: Crawled 1542 pages (at 115 pages/min), scraped 10056 items (at 550 items/min)\n",
      "2023-08-10 21:26:05 [scrapy.extensions.logstats] INFO: Crawled 1626 pages (at 84 pages/min), scraped 10527 items (at 471 items/min)\n",
      "2023-08-10 21:27:05 [scrapy.extensions.logstats] INFO: Crawled 1742 pages (at 116 pages/min), scraped 11169 items (at 642 items/min)\n",
      "2023-08-10 21:28:06 [scrapy.extensions.logstats] INFO: Crawled 1855 pages (at 113 pages/min), scraped 11805 items (at 636 items/min)\n",
      "2023-08-10 21:29:05 [scrapy.extensions.logstats] INFO: Crawled 1961 pages (at 106 pages/min), scraped 12418 items (at 613 items/min)\n",
      "2023-08-10 21:30:06 [scrapy.extensions.logstats] INFO: Crawled 2068 pages (at 107 pages/min), scraped 12990 items (at 572 items/min)\n",
      "2023-08-10 21:31:06 [scrapy.extensions.logstats] INFO: Crawled 2299 pages (at 231 pages/min), scraped 14854 items (at 1864 items/min)\n",
      "2023-08-10 21:32:06 [scrapy.extensions.logstats] INFO: Crawled 2561 pages (at 262 pages/min), scraped 17313 items (at 2459 items/min)\n",
      "2023-08-10 21:33:06 [scrapy.extensions.logstats] INFO: Crawled 2784 pages (at 223 pages/min), scraped 19485 items (at 2172 items/min)\n",
      "2023-08-10 21:34:06 [scrapy.extensions.logstats] INFO: Crawled 3038 pages (at 254 pages/min), scraped 21964 items (at 2479 items/min)\n",
      "2023-08-10 21:35:06 [scrapy.extensions.logstats] INFO: Crawled 3265 pages (at 227 pages/min), scraped 24222 items (at 2258 items/min)\n",
      "2023-08-10 21:36:06 [scrapy.extensions.logstats] INFO: Crawled 3458 pages (at 193 pages/min), scraped 26065 items (at 1843 items/min)\n",
      "2023-08-10 21:37:06 [scrapy.extensions.logstats] INFO: Crawled 3635 pages (at 177 pages/min), scraped 27757 items (at 1692 items/min)\n",
      "2023-08-10 21:38:05 [scrapy.extensions.logstats] INFO: Crawled 3788 pages (at 153 pages/min), scraped 28944 items (at 1187 items/min)\n",
      "2023-08-10 21:39:05 [scrapy.extensions.logstats] INFO: Crawled 3948 pages (at 160 pages/min), scraped 30220 items (at 1276 items/min)\n",
      "2023-08-10 21:40:05 [scrapy.extensions.logstats] INFO: Crawled 3997 pages (at 49 pages/min), scraped 30618 items (at 398 items/min)\n",
      "2023-08-10 21:41:06 [scrapy.extensions.logstats] INFO: Crawled 4057 pages (at 60 pages/min), scraped 31007 items (at 389 items/min)\n",
      "2023-08-10 21:42:05 [scrapy.extensions.logstats] INFO: Crawled 4128 pages (at 71 pages/min), scraped 31500 items (at 493 items/min)\n",
      "2023-08-10 21:43:06 [scrapy.extensions.logstats] INFO: Crawled 4188 pages (at 60 pages/min), scraped 31910 items (at 410 items/min)\n",
      "2023-08-10 21:44:06 [scrapy.extensions.logstats] INFO: Crawled 4258 pages (at 70 pages/min), scraped 32391 items (at 481 items/min)\n",
      "2023-08-10 21:45:05 [scrapy.extensions.logstats] INFO: Crawled 4331 pages (at 73 pages/min), scraped 32874 items (at 483 items/min)\n",
      "2023-08-10 21:46:05 [scrapy.extensions.logstats] INFO: Crawled 4471 pages (at 140 pages/min), scraped 33766 items (at 892 items/min)\n",
      "2023-08-10 21:47:06 [scrapy.extensions.logstats] INFO: Crawled 4559 pages (at 88 pages/min), scraped 34200 items (at 434 items/min)\n",
      "2023-08-10 21:48:05 [scrapy.extensions.logstats] INFO: Crawled 4614 pages (at 55 pages/min), scraped 34614 items (at 414 items/min)\n",
      "2023-08-10 21:49:06 [scrapy.extensions.logstats] INFO: Crawled 4694 pages (at 80 pages/min), scraped 35032 items (at 418 items/min)\n",
      "2023-08-10 21:50:06 [scrapy.extensions.logstats] INFO: Crawled 4772 pages (at 78 pages/min), scraped 35473 items (at 441 items/min)\n",
      "2023-08-10 21:51:06 [scrapy.extensions.logstats] INFO: Crawled 4887 pages (at 115 pages/min), scraped 36129 items (at 656 items/min)\n",
      "2023-08-10 21:52:05 [scrapy.extensions.logstats] INFO: Crawled 5012 pages (at 125 pages/min), scraped 37119 items (at 990 items/min)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 21:53:06 [scrapy.extensions.logstats] INFO: Crawled 5142 pages (at 130 pages/min), scraped 38033 items (at 914 items/min)\n",
      "2023-08-10 21:54:05 [scrapy.extensions.logstats] INFO: Crawled 5251 pages (at 109 pages/min), scraped 38750 items (at 717 items/min)\n",
      "2023-08-10 21:55:05 [scrapy.extensions.logstats] INFO: Crawled 5332 pages (at 81 pages/min), scraped 39148 items (at 398 items/min)\n",
      "2023-08-10 21:56:05 [scrapy.extensions.logstats] INFO: Crawled 5390 pages (at 58 pages/min), scraped 39457 items (at 309 items/min)\n",
      "2023-08-10 21:57:06 [scrapy.extensions.logstats] INFO: Crawled 5437 pages (at 47 pages/min), scraped 39646 items (at 189 items/min)\n",
      "2023-08-10 21:58:05 [scrapy.extensions.logstats] INFO: Crawled 5512 pages (at 75 pages/min), scraped 39944 items (at 298 items/min)\n",
      "2023-08-10 21:59:06 [scrapy.extensions.logstats] INFO: Crawled 5623 pages (at 111 pages/min), scraped 40563 items (at 619 items/min)\n",
      "2023-08-10 22:00:05 [scrapy.extensions.logstats] INFO: Crawled 5719 pages (at 96 pages/min), scraped 41066 items (at 503 items/min)\n",
      "2023-08-10 22:01:05 [scrapy.extensions.logstats] INFO: Crawled 5830 pages (at 111 pages/min), scraped 41705 items (at 639 items/min)\n",
      "2023-08-10 22:02:05 [scrapy.extensions.logstats] INFO: Crawled 5874 pages (at 44 pages/min), scraped 41815 items (at 110 items/min)\n",
      "2023-08-10 22:03:05 [scrapy.extensions.logstats] INFO: Crawled 5947 pages (at 73 pages/min), scraped 41983 items (at 168 items/min)\n",
      "2023-08-10 22:04:05 [scrapy.extensions.logstats] INFO: Crawled 6039 pages (at 92 pages/min), scraped 42335 items (at 352 items/min)\n",
      "2023-08-10 22:05:05 [scrapy.extensions.logstats] INFO: Crawled 6142 pages (at 103 pages/min), scraped 42848 items (at 513 items/min)\n",
      "2023-08-10 22:06:05 [scrapy.extensions.logstats] INFO: Crawled 6261 pages (at 119 pages/min), scraped 43411 items (at 563 items/min)\n",
      "2023-08-10 22:07:06 [scrapy.extensions.logstats] INFO: Crawled 6370 pages (at 109 pages/min), scraped 43811 items (at 400 items/min)\n",
      "2023-08-10 22:08:06 [scrapy.extensions.logstats] INFO: Crawled 6479 pages (at 109 pages/min), scraped 44241 items (at 430 items/min)\n",
      "2023-08-10 22:09:05 [scrapy.extensions.logstats] INFO: Crawled 6611 pages (at 132 pages/min), scraped 44832 items (at 591 items/min)\n",
      "2023-08-10 22:10:05 [scrapy.extensions.logstats] INFO: Crawled 6698 pages (at 87 pages/min), scraped 45131 items (at 299 items/min)\n",
      "2023-08-10 22:11:06 [scrapy.extensions.logstats] INFO: Crawled 6806 pages (at 108 pages/min), scraped 45470 items (at 339 items/min)\n",
      "2023-08-10 22:12:05 [scrapy.extensions.logstats] INFO: Crawled 6941 pages (at 135 pages/min), scraped 46158 items (at 688 items/min)\n",
      "2023-08-10 22:13:05 [scrapy.extensions.logstats] INFO: Crawled 7120 pages (at 179 pages/min), scraped 47249 items (at 1091 items/min)\n",
      "2023-08-10 22:14:05 [scrapy.extensions.logstats] INFO: Crawled 7213 pages (at 93 pages/min), scraped 47813 items (at 564 items/min)\n",
      "2023-08-10 22:15:05 [scrapy.extensions.logstats] INFO: Crawled 7348 pages (at 135 pages/min), scraped 48656 items (at 843 items/min)\n",
      "2023-08-10 22:16:05 [scrapy.extensions.logstats] INFO: Crawled 7462 pages (at 114 pages/min), scraped 49354 items (at 698 items/min)\n",
      "2023-08-10 22:17:05 [scrapy.extensions.logstats] INFO: Crawled 7583 pages (at 121 pages/min), scraped 50045 items (at 691 items/min)\n",
      "2023-08-10 22:18:06 [scrapy.extensions.logstats] INFO: Crawled 7679 pages (at 96 pages/min), scraped 50640 items (at 595 items/min)\n",
      "2023-08-10 22:19:05 [scrapy.extensions.logstats] INFO: Crawled 7761 pages (at 82 pages/min), scraped 51162 items (at 522 items/min)\n",
      "2023-08-10 22:20:05 [scrapy.extensions.logstats] INFO: Crawled 7846 pages (at 85 pages/min), scraped 51612 items (at 450 items/min)\n",
      "2023-08-10 22:21:06 [scrapy.extensions.logstats] INFO: Crawled 7971 pages (at 125 pages/min), scraped 52295 items (at 683 items/min)\n",
      "2023-08-10 22:22:06 [scrapy.extensions.logstats] INFO: Crawled 8128 pages (at 157 pages/min), scraped 53472 items (at 1177 items/min)\n",
      "2023-08-10 22:23:05 [scrapy.extensions.logstats] INFO: Crawled 8298 pages (at 170 pages/min), scraped 54840 items (at 1368 items/min)\n",
      "2023-08-10 22:24:06 [scrapy.extensions.logstats] INFO: Crawled 8478 pages (at 180 pages/min), scraped 56201 items (at 1361 items/min)\n",
      "2023-08-10 22:25:05 [scrapy.extensions.logstats] INFO: Crawled 8628 pages (at 150 pages/min), scraped 57347 items (at 1146 items/min)\n",
      "2023-08-10 22:26:06 [scrapy.extensions.logstats] INFO: Crawled 8760 pages (at 132 pages/min), scraped 58249 items (at 902 items/min)\n",
      "2023-08-10 22:27:05 [scrapy.extensions.logstats] INFO: Crawled 8881 pages (at 121 pages/min), scraped 58985 items (at 736 items/min)\n",
      "2023-08-10 22:28:06 [scrapy.extensions.logstats] INFO: Crawled 8928 pages (at 47 pages/min), scraped 59301 items (at 316 items/min)\n",
      "2023-08-10 22:29:06 [scrapy.extensions.logstats] INFO: Crawled 8952 pages (at 24 pages/min), scraped 59421 items (at 120 items/min)\n",
      "2023-08-10 22:30:05 [scrapy.extensions.logstats] INFO: Crawled 8960 pages (at 8 pages/min), scraped 59442 items (at 21 items/min)\n",
      "2023-08-10 22:31:05 [scrapy.extensions.logstats] INFO: Crawled 8975 pages (at 15 pages/min), scraped 59543 items (at 101 items/min)\n",
      "2023-08-10 22:32:05 [scrapy.extensions.logstats] INFO: Crawled 9064 pages (at 89 pages/min), scraped 60052 items (at 509 items/min)\n",
      "2023-08-10 22:33:06 [scrapy.extensions.logstats] INFO: Crawled 9167 pages (at 103 pages/min), scraped 60681 items (at 629 items/min)\n",
      "2023-08-10 22:34:05 [scrapy.extensions.logstats] INFO: Crawled 9239 pages (at 72 pages/min), scraped 61095 items (at 414 items/min)\n",
      "2023-08-10 22:35:05 [scrapy.extensions.logstats] INFO: Crawled 9327 pages (at 88 pages/min), scraped 61553 items (at 458 items/min)\n",
      "2023-08-10 22:36:05 [scrapy.extensions.logstats] INFO: Crawled 9391 pages (at 64 pages/min), scraped 61825 items (at 272 items/min)\n",
      "2023-08-10 22:37:06 [scrapy.extensions.logstats] INFO: Crawled 9451 pages (at 60 pages/min), scraped 62202 items (at 377 items/min)\n",
      "2023-08-10 22:38:06 [scrapy.extensions.logstats] INFO: Crawled 9526 pages (at 75 pages/min), scraped 62942 items (at 740 items/min)\n",
      "2023-08-10 22:39:06 [scrapy.extensions.logstats] INFO: Crawled 9586 pages (at 60 pages/min), scraped 63497 items (at 555 items/min)\n",
      "2023-08-10 22:40:09 [scrapy.extensions.logstats] INFO: Crawled 9629 pages (at 43 pages/min), scraped 63925 items (at 428 items/min)\n",
      "2023-08-10 22:41:06 [scrapy.extensions.logstats] INFO: Crawled 9715 pages (at 86 pages/min), scraped 64683 items (at 758 items/min)\n",
      "2023-08-10 22:42:06 [scrapy.extensions.logstats] INFO: Crawled 9838 pages (at 123 pages/min), scraped 65831 items (at 1148 items/min)\n",
      "2023-08-10 22:43:06 [scrapy.extensions.logstats] INFO: Crawled 9916 pages (at 78 pages/min), scraped 66598 items (at 767 items/min)\n",
      "2023-08-10 22:44:06 [scrapy.extensions.logstats] INFO: Crawled 10004 pages (at 88 pages/min), scraped 67446 items (at 848 items/min)\n",
      "2023-08-10 22:45:06 [scrapy.extensions.logstats] INFO: Crawled 10089 pages (at 85 pages/min), scraped 68275 items (at 829 items/min)\n",
      "2023-08-10 22:46:05 [scrapy.extensions.logstats] INFO: Crawled 10185 pages (at 96 pages/min), scraped 69203 items (at 928 items/min)\n",
      "2023-08-10 22:47:06 [scrapy.extensions.logstats] INFO: Crawled 10315 pages (at 130 pages/min), scraped 70411 items (at 1208 items/min)\n",
      "2023-08-10 22:48:06 [scrapy.extensions.logstats] INFO: Crawled 10421 pages (at 106 pages/min), scraped 71407 items (at 996 items/min)\n",
      "2023-08-10 22:49:06 [scrapy.extensions.logstats] INFO: Crawled 10533 pages (at 112 pages/min), scraped 72521 items (at 1114 items/min)\n",
      "2023-08-10 22:50:06 [scrapy.extensions.logstats] INFO: Crawled 10656 pages (at 123 pages/min), scraped 73685 items (at 1164 items/min)\n",
      "2023-08-10 22:51:06 [scrapy.extensions.logstats] INFO: Crawled 10776 pages (at 120 pages/min), scraped 74881 items (at 1196 items/min)\n",
      "2023-08-10 22:52:06 [scrapy.extensions.logstats] INFO: Crawled 10877 pages (at 101 pages/min), scraped 75790 items (at 909 items/min)\n",
      "2023-08-10 22:53:06 [scrapy.extensions.logstats] INFO: Crawled 10971 pages (at 94 pages/min), scraped 76667 items (at 877 items/min)\n",
      "2023-08-10 22:54:06 [scrapy.extensions.logstats] INFO: Crawled 11060 pages (at 89 pages/min), scraped 77519 items (at 852 items/min)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 22:55:06 [scrapy.extensions.logstats] INFO: Crawled 11160 pages (at 100 pages/min), scraped 78435 items (at 916 items/min)\n",
      "2023-08-10 22:56:06 [scrapy.extensions.logstats] INFO: Crawled 11280 pages (at 120 pages/min), scraped 79617 items (at 1182 items/min)\n",
      "2023-08-10 22:57:06 [scrapy.extensions.logstats] INFO: Crawled 11406 pages (at 126 pages/min), scraped 80819 items (at 1202 items/min)\n",
      "2023-08-10 22:58:06 [scrapy.extensions.logstats] INFO: Crawled 11539 pages (at 133 pages/min), scraped 82018 items (at 1199 items/min)\n",
      "2023-08-10 22:59:06 [scrapy.extensions.logstats] INFO: Crawled 11683 pages (at 144 pages/min), scraped 83357 items (at 1339 items/min)\n",
      "2023-08-10 23:00:06 [scrapy.extensions.logstats] INFO: Crawled 11820 pages (at 137 pages/min), scraped 84639 items (at 1282 items/min)\n",
      "2023-08-10 23:01:06 [scrapy.extensions.logstats] INFO: Crawled 11964 pages (at 144 pages/min), scraped 85964 items (at 1325 items/min)\n",
      "2023-08-10 23:02:06 [scrapy.extensions.logstats] INFO: Crawled 12095 pages (at 131 pages/min), scraped 87168 items (at 1204 items/min)\n",
      "2023-08-10 23:03:06 [scrapy.extensions.logstats] INFO: Crawled 12218 pages (at 123 pages/min), scraped 88274 items (at 1106 items/min)\n",
      "2023-08-10 23:04:06 [scrapy.extensions.logstats] INFO: Crawled 12357 pages (at 139 pages/min), scraped 89510 items (at 1236 items/min)\n",
      "2023-08-10 23:05:06 [scrapy.extensions.logstats] INFO: Crawled 12492 pages (at 135 pages/min), scraped 90732 items (at 1222 items/min)\n",
      "2023-08-10 23:06:06 [scrapy.extensions.logstats] INFO: Crawled 12618 pages (at 126 pages/min), scraped 91832 items (at 1100 items/min)\n",
      "2023-08-10 23:07:06 [scrapy.extensions.logstats] INFO: Crawled 12752 pages (at 134 pages/min), scraped 93053 items (at 1221 items/min)\n",
      "2023-08-10 23:08:06 [scrapy.extensions.logstats] INFO: Crawled 12895 pages (at 143 pages/min), scraped 94407 items (at 1354 items/min)\n",
      "2023-08-10 23:09:06 [scrapy.extensions.logstats] INFO: Crawled 13058 pages (at 163 pages/min), scraped 95855 items (at 1448 items/min)\n",
      "2023-08-10 23:10:06 [scrapy.extensions.logstats] INFO: Crawled 13208 pages (at 150 pages/min), scraped 97248 items (at 1393 items/min)\n",
      "2023-08-10 23:11:06 [scrapy.extensions.logstats] INFO: Crawled 13373 pages (at 165 pages/min), scraped 98623 items (at 1375 items/min)\n",
      "2023-08-10 23:12:06 [scrapy.extensions.logstats] INFO: Crawled 13541 pages (at 168 pages/min), scraped 100131 items (at 1508 items/min)\n",
      "2023-08-10 23:13:22 [scrapy.extensions.logstats] INFO: Crawled 13644 pages (at 103 pages/min), scraped 101015 items (at 884 items/min)\n",
      "2023-08-10 23:14:07 [scrapy.extensions.logstats] INFO: Crawled 13656 pages (at 12 pages/min), scraped 101130 items (at 115 items/min)\n",
      "2023-08-10 23:15:09 [scrapy.extensions.logstats] INFO: Crawled 13679 pages (at 23 pages/min), scraped 101310 items (at 180 items/min)\n",
      "2023-08-10 23:16:07 [scrapy.extensions.logstats] INFO: Crawled 13697 pages (at 18 pages/min), scraped 101483 items (at 173 items/min)\n",
      "2023-08-10 23:17:06 [scrapy.extensions.logstats] INFO: Crawled 13721 pages (at 24 pages/min), scraped 101712 items (at 229 items/min)\n",
      "2023-08-10 23:18:06 [scrapy.extensions.logstats] INFO: Crawled 13754 pages (at 33 pages/min), scraped 101998 items (at 286 items/min)\n",
      "2023-08-10 23:19:05 [scrapy.extensions.logstats] INFO: Crawled 13825 pages (at 71 pages/min), scraped 102678 items (at 680 items/min)\n",
      "2023-08-10 23:20:05 [scrapy.extensions.logstats] INFO: Crawled 13973 pages (at 148 pages/min), scraped 103881 items (at 1203 items/min)\n",
      "2023-08-10 23:21:05 [scrapy.extensions.logstats] INFO: Crawled 14095 pages (at 122 pages/min), scraped 104969 items (at 1088 items/min)\n",
      "2023-08-10 23:22:05 [scrapy.extensions.logstats] INFO: Crawled 14165 pages (at 70 pages/min), scraped 105658 items (at 689 items/min)\n",
      "2023-08-10 23:23:05 [scrapy.extensions.logstats] INFO: Crawled 14222 pages (at 57 pages/min), scraped 106228 items (at 570 items/min)\n",
      "2023-08-10 23:24:05 [scrapy.extensions.logstats] INFO: Crawled 14276 pages (at 54 pages/min), scraped 106758 items (at 530 items/min)\n",
      "2023-08-10 23:24:59 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-08-10 23:24:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 11827133,\n",
      " 'downloader/request_count': 17853,\n",
      " 'downloader/request_method_count/GET': 17853,\n",
      " 'downloader/response_bytes': 2048257975,\n",
      " 'downloader/response_count': 17853,\n",
      " 'downloader/response_status_count/200': 14305,\n",
      " 'downloader/response_status_count/301': 3548,\n",
      " 'dupefilter/filtered': 1364,\n",
      " 'elapsed_time_seconds': 7913.660159,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 8, 10, 20, 24, 59, 644162),\n",
      " 'httpcache/firsthand': 12505,\n",
      " 'httpcache/hit': 5348,\n",
      " 'httpcache/miss': 12505,\n",
      " 'httpcache/store': 12505,\n",
      " 'httpcompression/response_bytes': 14300111960,\n",
      " 'httpcompression/response_count': 14305,\n",
      " 'item_scraped_count': 107045,\n",
      " 'log_count/INFO': 141,\n",
      " 'log_count/WARNING': 1,\n",
      " 'request_depth_max': 534,\n",
      " 'response_received_count': 14305,\n",
      " 'scheduler/dequeued': 17853,\n",
      " 'scheduler/dequeued/memory': 17853,\n",
      " 'scheduler/enqueued': 17853,\n",
      " 'scheduler/enqueued/memory': 17853,\n",
      " 'start_time': datetime.datetime(2023, 8, 10, 18, 13, 5, 984003)}\n",
      "2023-08-10 23:24:59 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "class DermStoreSpider (scrapy.Spider):\n",
    "    \"\"\"\n",
    "    A Scrapy Spider to scrape skin care products and reviews from DermStore website.\n",
    "    \n",
    "    Attributes:\n",
    "        name (str): The name of the Spider.\n",
    "        url (str): The URL of the product listing page\n",
    "    \"\"\"\n",
    "    name = \"skin_care\"\n",
    "    url = \"https://www.dermstore.com/skin-care.list\"\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \"\"\"\n",
    "        Generates initial requests to start scraping.\n",
    "        \n",
    "        returns:\n",
    "            scrapy.Request: A request to the URL specified in 'url' attribute.\n",
    "        \"\"\"\n",
    "        #here we define the links that we'll scrape\n",
    "        start_urls = ['https://www.dermstore.com/skin-care.list']\n",
    "        for url in start_urls:\n",
    "            yield scrapy.Request(url = url, callback = self.parse_current_page)\n",
    "            \n",
    "    def parse_current_page(self, response):\n",
    "        \"\"\"\"    \n",
    "        Method for pagination. Accept the start page's content and extract the number of total pages in this category. \n",
    "        It then starts the loop, where it defines the current page and follows it for further extraction in the\n",
    "        parse_front() method.\n",
    "        \n",
    "        args:\n",
    "            response (scrapy.http.Response): The response object containing the page content.\n",
    "        \n",
    "        returns:\n",
    "            a request object for the current url with products\n",
    "        \"\"\"\n",
    "        #define the number of total pages\n",
    "        total_pages = response.xpath('//nav[@class=\"responsivePaginationPages\"]/@data-total-pages').get()\n",
    "        #define the current page for extracting and follow it \n",
    "        for page in range(1, int(total_pages) + 1):\n",
    "            next_page_url = self.url + '?pageNumber=' + str(page)\n",
    "            yield response.follow(url = next_page_url, callback=self.parse_front)\n",
    "    \n",
    "    def parse_front(self, response):\n",
    "        \"\"\"\n",
    "        Accept the content from current page. It extracts the links for each product displayed on current page and follows\n",
    "        it for further extraction in the parse_pages() method\n",
    "        \n",
    "        args:\n",
    "            response (scrapy.http.Response): The response object containing the page content.\n",
    "            \n",
    "        returns:\n",
    "            a request object for the product's url\n",
    "        \"\"\"\n",
    "        #define the all products on the current page\n",
    "        product_block = response.css('div.productBlock')\n",
    "        \n",
    "        #extract the link to the product page and follow it for further scraping\n",
    "        for product in product_block:\n",
    "            attr = product.css('a::attr(href)').get()\n",
    "            product_url = 'www.dermstore.com' + attr\n",
    "            yield response.follow(url=product_url, callback=self.parse_pages, meta={\"product_url\": product_url})\n",
    "        \n",
    "    def parse_pages(self, response):\n",
    "        \"\"\"\n",
    "        Accept the content from the product page and extract the product name, product ingredients and the product url \n",
    "        as meta data from the function originated the request. Define the number of total reviews. \n",
    "        Depending on the number it proceed to next steps in follow logic:\n",
    "        - if there are more than 10 reviews for product it extracts the link of all reviews and follows it for next \n",
    "        extraction in the parse_review() function\n",
    "        - if there are less or equal 10 reviews it extracts the review date, review title, review body, \n",
    "        rating and returns the dictionary containig the scraped data\n",
    "        \n",
    "        args: \n",
    "            response (scrapy.http.Response): The response object containing the product page content and meta data\n",
    "            from the function that originated the request.\n",
    "            \n",
    "        returns: \n",
    "            depends on number of reviews: \n",
    "            - a dictionary containing the scraped data \n",
    "            - a request object for the review's url\n",
    "        \"\"\"\n",
    "        #extract the product name, product ingredients\n",
    "        product_name = response.xpath('//h1[contains(@class,\"productName_title\")]/text()').get()\n",
    "        product_ingredients_list = response.xpath('//*[@id=\"product-description-content-7\"]/div/div/p/text()').getall() \n",
    "        product_ingredients = ' '.join(product_ingredients_list).strip()\n",
    "        #access meta data\n",
    "        product_url = response.request.meta['product_url']\n",
    "            \n",
    "        #define the number of total reviews\n",
    "        total_reviews = response.xpath('//p[@class=\"athenaProductReviews_reviewCount Auto\"]/text()').get()\n",
    "        #define the logic of next steps if the reviews exists\n",
    "        if total_reviews:\n",
    "            total_reviews = int(total_reviews.replace(' Reviews', ''))\n",
    "            #the logic when there are more than 10 reviews\n",
    "            if total_reviews >= 10:\n",
    "                #extract a link to the reviews page\n",
    "                review_links = response.xpath('//a[contains(@class,\"athenaProductReviews_seeReviewsButton\")]/@href').getall()\n",
    "                #follow the link to the reviews page for further scraping\n",
    "                for next_review in review_links:\n",
    "                    yield response.follow(url=next_review, callback=self.parse_review, \n",
    "                                          meta={\"product_name\": product_name,\n",
    "                                                \"product_ingredients\": product_ingredients, \n",
    "                                                \"product_url\": response.url})\n",
    "            #the logic when there are less or equal 10 reviews\n",
    "            else:\n",
    "                #access meta data\n",
    "                product_url = response.request.meta['product_url']\n",
    "                #define reviews\n",
    "                reviews = response.xpath('//div[@class=\"athenaProductReviews_topReviewSingle\"]')\n",
    "                #extract the review date, review title, review body and rating\n",
    "                for review in reviews:\n",
    "                    review_date = review.xpath(\".//span[@data-js-element='createdDate']/text()\").get()\n",
    "                    review_title = review.xpath(\".//h3[@id='product-review-1-title']/text()\").get()\n",
    "                    review_body = review.xpath(\".//p[@class='athenaProductReviews_topReviewsExcerpt']/text()\").get().strip('\\n ')\n",
    "                    rating = float(\n",
    "                        review.xpath('.//div[@class=\"athenaProductReviews_topReviewsRatingStarsContainer\"]/@aria-label')\n",
    "                        .get().replace(' Stars', ''))\n",
    "                    \n",
    "                    #yield the scraped data\n",
    "                    item = {\n",
    "                        'product_name': product_name if product_name else None, \n",
    "                        'product_url': product_url if product_url else None,\n",
    "                        'product_ingredients': product_ingredients if product_ingredients else None,\n",
    "                        'review_date': review_date if review_date else None,\n",
    "                        'review_title': review_title.strip('\\n ') if review_title else None,\n",
    "                        'review_body': review_body if review_body else None,\n",
    "                        'rating': rating if rating else None\n",
    "                    }\n",
    "                    yield item\n",
    "        else:\n",
    "            #yeild the scraped data\n",
    "            item = {\n",
    "                'product_name': product_name if product_name else None, \n",
    "                'product_url': product_url if product_url else None,\n",
    "                'product_ingredients': product_ingredients if product_ingredients else None,\n",
    "                'review_date': None,\n",
    "                'review_title': None,\n",
    "                'review_body': None,\n",
    "                'rating': None\n",
    "            }\n",
    "            yield item\n",
    "        \n",
    "    def parse_review(self, response):\n",
    "        \"\"\"\n",
    "        Accepts the content of the reviews page and the meta data from the function that originated the request.\n",
    "        Extracts the date, title, body and rating from each review.\n",
    "        Follows a link to the next page to repeat the process (if there is a next page)\n",
    "        \n",
    "        args: \n",
    "            response: holds the url page's content and meta data from the function that originated the request.\n",
    "            \n",
    "        returns:\n",
    "            a dictionary containing the scraped data for each review\n",
    "        \"\"\"\n",
    "        #access the meta data\n",
    "        product_name = response.request.meta['product_name']\n",
    "        product_url = response.request.meta['product_url']\n",
    "        product_ingredients = response.request.meta['product_ingredients']\n",
    "        \n",
    "        #define all reviews on the page\n",
    "        reviews = response.xpath('//div[@class = \"athenaProductReviews_review\"]')\n",
    "        \n",
    "        #extract the date, title, body and rating for each review\n",
    "        for review in reviews:\n",
    "            review_date = review.xpath(\".//div/span[@data-js-element='createdDate']/text()\").get()\n",
    "            review_title = review.xpath('.//h3[@class=\"athenaProductReviews_reviewTitle\"]/text()').get()\n",
    "            review_body = review.xpath('.//p[@class=\"athenaProductReviews_reviewContent\"]/text()').get().strip('\\n ')\n",
    "            rating = float(review.xpath('.//span[@class=\"athenaProductReviews_schemaRatingValue\"]/text()').get())\n",
    "\n",
    "            #yield the scraped data\n",
    "            item = {\n",
    "                'product_name': product_name if product_name else None, \n",
    "                'product_url': product_url if product_url else None,\n",
    "                'product_ingredients': product_ingredients if product_ingredients else None,\n",
    "                'review_date': review_date if review_date else None,\n",
    "                'review_title': review_title if review_title else None,\n",
    "                'review_body': review_body if review_body else None,\n",
    "                'rating': rating if rating else None\n",
    "            }\n",
    "            \n",
    "            yield item\n",
    "        \n",
    "        #pagination for a single product's review\n",
    "        next_page_review = response.xpath('//a[@aria-label=\"Next page\"]/@href').get()\n",
    "        #if a next page link exists, follow and repeat the scraping procedure\n",
    "        if next_page_review:\n",
    "            yield response.follow(url=str(next_page_review), callback=self.parse_review,\n",
    "                                     meta={\"product_name\": product_name,\n",
    "                                          \"product_ingredients\": product_ingredients, \n",
    "                                           \"product_url\": product_url})\n",
    "\n",
    "#define the empty DataFrame               \n",
    "df_dermStore = pd.DataFrame(\n",
    "    columns=['product_name', 'product_url', 'product_ingredients', 'review_date', 'review_title', 'review_body', \n",
    "             'rating'])\n",
    "\n",
    "#define the Class for Pipeline\n",
    "class DermStorePipeline:\n",
    "    \"\"\"\"\n",
    "    A Scrapy pipeline class to process and store scraped data into a pandas DataFrame.\n",
    "    \n",
    "    \"\"\"\n",
    "    #function for filling the empty DataFrame by yielded data\n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"\n",
    "        Processes the scraped item and stores it in the DataFrame.\n",
    "        \n",
    "        Attributes:\n",
    "            item (dict): The scraped data item.\n",
    "            spider (scrapy.Spider): The Spider instance.\n",
    "            \n",
    "        returns:\n",
    "            dict: The processed item.\n",
    "        \"\"\"\n",
    "        df_dermStore.loc[len(df_dermStore)] = [item['product_name'], item['product_url'], item['product_ingredients'], \n",
    "                                               item['review_date'], item['review_title'], item['review_body'], item['rating']]\n",
    "        return item\n",
    "\n",
    "process = CrawlerProcess(\n",
    "    settings={'ITEM_PIPELINES': {'__main__.DermStorePipeline': 300}, \n",
    "              'LOG_LEVEL': 'INFO', \n",
    "              'AUTOTHROTTLE_ENABLED': 'True',\n",
    "             'AUTOTHROTTLE_TARGET_CONCURRENCY':'1.0',\n",
    "             'HTTPCACHE_ENABLED': 'True'})\n",
    "process.crawl(DermStoreSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd3ad9",
   "metadata": {},
   "source": [
    "Let's take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8303d306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_url</th>\n",
       "      <th>product_ingredients</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_body</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Replenix Lifting Firming Neck Cream (1.7 fl. oz.)</td>\n",
       "      <td>https://www.dermstore.com/replenix-lifting-and...</td>\n",
       "      <td>Purified Water, Caprylic/Capric Triglyceride, ...</td>\n",
       "      <td>10/30/22</td>\n",
       "      <td>Good Firming Neck Cream</td>\n",
       "      <td>I keep trying new neck creams.  As an older wo...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Replenix Lifting Firming Neck Cream (1.7 fl. oz.)</td>\n",
       "      <td>https://www.dermstore.com/replenix-lifting-and...</td>\n",
       "      <td>Purified Water, Caprylic/Capric Triglyceride, ...</td>\n",
       "      <td>10/15/22</td>\n",
       "      <td>liked it</td>\n",
       "      <td>I liked this neck cream... I am not sure it re...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Replenix Lifting Firming Neck Cream (1.7 fl. oz.)</td>\n",
       "      <td>https://www.dermstore.com/replenix-lifting-and...</td>\n",
       "      <td>Purified Water, Caprylic/Capric Triglyceride, ...</td>\n",
       "      <td>9/10/22</td>\n",
       "      <td>My new holy grail</td>\n",
       "      <td>The neck cream has so many of the best ingredi...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Replenix Lifting Firming Neck Cream (1.7 fl. oz.)</td>\n",
       "      <td>https://www.dermstore.com/replenix-lifting-and...</td>\n",
       "      <td>Purified Water, Caprylic/Capric Triglyceride, ...</td>\n",
       "      <td>4/24/22</td>\n",
       "      <td>I see a difference with consistent use</td>\n",
       "      <td>I see a difference with consistent use. It too...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Replenix Lifting Firming Neck Cream (1.7 fl. oz.)</td>\n",
       "      <td>https://www.dermstore.com/replenix-lifting-and...</td>\n",
       "      <td>Purified Water, Caprylic/Capric Triglyceride, ...</td>\n",
       "      <td>4/3/22</td>\n",
       "      <td>Fantastic neck cream</td>\n",
       "      <td>My neck has held up well and I am in my 60's. ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107040</th>\n",
       "      <td>SkinCeuticals C E Ferulic (1 fl. oz.)</td>\n",
       "      <td>https://www.dermstore.com/skinceuticals-c-e-fe...</td>\n",
       "      <td>Aqua/Water/Eau, Ethoxydiglycol, Ascorbic Acid,...</td>\n",
       "      <td>3/22/06</td>\n",
       "      <td>Wrinkle free serum</td>\n",
       "      <td>I have used this serum for several years and k...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107041</th>\n",
       "      <td>SkinCeuticals C E Ferulic (1 fl. oz.)</td>\n",
       "      <td>https://www.dermstore.com/skinceuticals-c-e-fe...</td>\n",
       "      <td>Aqua/Water/Eau, Ethoxydiglycol, Ascorbic Acid,...</td>\n",
       "      <td>2/8/21</td>\n",
       "      <td>Slow but steady</td>\n",
       "      <td>I have been using this in collaboration with S...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107042</th>\n",
       "      <td>SkinCeuticals C E Ferulic (1 fl. oz.)</td>\n",
       "      <td>https://www.dermstore.com/skinceuticals-c-e-fe...</td>\n",
       "      <td>Aqua/Water/Eau, Ethoxydiglycol, Ascorbic Acid,...</td>\n",
       "      <td>2/7/21</td>\n",
       "      <td>Awesome product</td>\n",
       "      <td>This leaves your face feeling so good! It has ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107043</th>\n",
       "      <td>SkinCeuticals C E Ferulic (1 fl. oz.)</td>\n",
       "      <td>https://www.dermstore.com/skinceuticals-c-e-fe...</td>\n",
       "      <td>Aqua/Water/Eau, Ethoxydiglycol, Ascorbic Acid,...</td>\n",
       "      <td>2/7/21</td>\n",
       "      <td>Vitamin c potency</td>\n",
       "      <td>This has very potent vitamin c in it, and it a...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107044</th>\n",
       "      <td>SkinCeuticals C E Ferulic (1 fl. oz.)</td>\n",
       "      <td>https://www.dermstore.com/skinceuticals-c-e-fe...</td>\n",
       "      <td>Aqua/Water/Eau, Ethoxydiglycol, Ascorbic Acid,...</td>\n",
       "      <td>2/4/21</td>\n",
       "      <td>tingly radiance</td>\n",
       "      <td>From the very first time I used this, people a...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107045 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             product_name  \\\n",
       "0       Replenix Lifting Firming Neck Cream (1.7 fl. oz.)   \n",
       "1       Replenix Lifting Firming Neck Cream (1.7 fl. oz.)   \n",
       "2       Replenix Lifting Firming Neck Cream (1.7 fl. oz.)   \n",
       "3       Replenix Lifting Firming Neck Cream (1.7 fl. oz.)   \n",
       "4       Replenix Lifting Firming Neck Cream (1.7 fl. oz.)   \n",
       "...                                                   ...   \n",
       "107040              SkinCeuticals C E Ferulic (1 fl. oz.)   \n",
       "107041              SkinCeuticals C E Ferulic (1 fl. oz.)   \n",
       "107042              SkinCeuticals C E Ferulic (1 fl. oz.)   \n",
       "107043              SkinCeuticals C E Ferulic (1 fl. oz.)   \n",
       "107044              SkinCeuticals C E Ferulic (1 fl. oz.)   \n",
       "\n",
       "                                              product_url  \\\n",
       "0       https://www.dermstore.com/replenix-lifting-and...   \n",
       "1       https://www.dermstore.com/replenix-lifting-and...   \n",
       "2       https://www.dermstore.com/replenix-lifting-and...   \n",
       "3       https://www.dermstore.com/replenix-lifting-and...   \n",
       "4       https://www.dermstore.com/replenix-lifting-and...   \n",
       "...                                                   ...   \n",
       "107040  https://www.dermstore.com/skinceuticals-c-e-fe...   \n",
       "107041  https://www.dermstore.com/skinceuticals-c-e-fe...   \n",
       "107042  https://www.dermstore.com/skinceuticals-c-e-fe...   \n",
       "107043  https://www.dermstore.com/skinceuticals-c-e-fe...   \n",
       "107044  https://www.dermstore.com/skinceuticals-c-e-fe...   \n",
       "\n",
       "                                      product_ingredients review_date  \\\n",
       "0       Purified Water, Caprylic/Capric Triglyceride, ...    10/30/22   \n",
       "1       Purified Water, Caprylic/Capric Triglyceride, ...    10/15/22   \n",
       "2       Purified Water, Caprylic/Capric Triglyceride, ...     9/10/22   \n",
       "3       Purified Water, Caprylic/Capric Triglyceride, ...     4/24/22   \n",
       "4       Purified Water, Caprylic/Capric Triglyceride, ...      4/3/22   \n",
       "...                                                   ...         ...   \n",
       "107040  Aqua/Water/Eau, Ethoxydiglycol, Ascorbic Acid,...     3/22/06   \n",
       "107041  Aqua/Water/Eau, Ethoxydiglycol, Ascorbic Acid,...      2/8/21   \n",
       "107042  Aqua/Water/Eau, Ethoxydiglycol, Ascorbic Acid,...      2/7/21   \n",
       "107043  Aqua/Water/Eau, Ethoxydiglycol, Ascorbic Acid,...      2/7/21   \n",
       "107044  Aqua/Water/Eau, Ethoxydiglycol, Ascorbic Acid,...      2/4/21   \n",
       "\n",
       "                                  review_title  \\\n",
       "0                      Good Firming Neck Cream   \n",
       "1                                     liked it   \n",
       "2                            My new holy grail   \n",
       "3       I see a difference with consistent use   \n",
       "4                         Fantastic neck cream   \n",
       "...                                        ...   \n",
       "107040                      Wrinkle free serum   \n",
       "107041                         Slow but steady   \n",
       "107042                         Awesome product   \n",
       "107043                       Vitamin c potency   \n",
       "107044                         tingly radiance   \n",
       "\n",
       "                                              review_body  rating  \n",
       "0       I keep trying new neck creams.  As an older wo...     5.0  \n",
       "1       I liked this neck cream... I am not sure it re...     4.0  \n",
       "2       The neck cream has so many of the best ingredi...     5.0  \n",
       "3       I see a difference with consistent use. It too...     5.0  \n",
       "4       My neck has held up well and I am in my 60's. ...     5.0  \n",
       "...                                                   ...     ...  \n",
       "107040  I have used this serum for several years and k...     4.0  \n",
       "107041  I have been using this in collaboration with S...     5.0  \n",
       "107042  This leaves your face feeling so good! It has ...     5.0  \n",
       "107043  This has very potent vitamin c in it, and it a...     5.0  \n",
       "107044  From the very first time I used this, people a...     5.0  \n",
       "\n",
       "[107045 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dermStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbc9a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3517"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dermStore['product_name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b341d7",
   "metadata": {},
   "source": [
    "Let's put the data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c32435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dermStore.to_csv('C:/Users/Dell/Documents/For project/MeNow!/web scraping project/dermstore/skin_care.csv', \n",
    "                    index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3d79cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_4e350_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >missing</th>\n",
       "      <th class=\"col_heading level0 col1\" >total</th>\n",
       "      <th class=\"col_heading level0 col2\" >percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4e350_level0_row0\" class=\"row_heading level0 row0\" >review_title</th>\n",
       "      <td id=\"T_4e350_row0_col0\" class=\"data row0 col0\" >4,316</td>\n",
       "      <td id=\"T_4e350_row0_col1\" class=\"data row0 col1\" >107,045</td>\n",
       "      <td id=\"T_4e350_row0_col2\" class=\"data row0 col2\" >4.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e350_level0_row1\" class=\"row_heading level0 row1\" >review_date</th>\n",
       "      <td id=\"T_4e350_row1_col0\" class=\"data row1 col0\" >1,206</td>\n",
       "      <td id=\"T_4e350_row1_col1\" class=\"data row1 col1\" >107,045</td>\n",
       "      <td id=\"T_4e350_row1_col2\" class=\"data row1 col2\" >1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e350_level0_row2\" class=\"row_heading level0 row2\" >review_body</th>\n",
       "      <td id=\"T_4e350_row2_col0\" class=\"data row2 col0\" >1,206</td>\n",
       "      <td id=\"T_4e350_row2_col1\" class=\"data row2 col1\" >107,045</td>\n",
       "      <td id=\"T_4e350_row2_col2\" class=\"data row2 col2\" >1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e350_level0_row3\" class=\"row_heading level0 row3\" >rating</th>\n",
       "      <td id=\"T_4e350_row3_col0\" class=\"data row3 col0\" >1,206</td>\n",
       "      <td id=\"T_4e350_row3_col1\" class=\"data row3 col1\" >107,045</td>\n",
       "      <td id=\"T_4e350_row3_col2\" class=\"data row3 col2\" >1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e350_level0_row4\" class=\"row_heading level0 row4\" >product_ingredients</th>\n",
       "      <td id=\"T_4e350_row4_col0\" class=\"data row4 col0\" >364</td>\n",
       "      <td id=\"T_4e350_row4_col1\" class=\"data row4 col1\" >107,045</td>\n",
       "      <td id=\"T_4e350_row4_col2\" class=\"data row4 col2\" >0.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e350_level0_row5\" class=\"row_heading level0 row5\" >product_name</th>\n",
       "      <td id=\"T_4e350_row5_col0\" class=\"data row5 col0\" >6</td>\n",
       "      <td id=\"T_4e350_row5_col1\" class=\"data row5 col1\" >107,045</td>\n",
       "      <td id=\"T_4e350_row5_col2\" class=\"data row5 col2\" >0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e350_level0_row6\" class=\"row_heading level0 row6\" >product_url</th>\n",
       "      <td id=\"T_4e350_row6_col0\" class=\"data row6 col0\" >0</td>\n",
       "      <td id=\"T_4e350_row6_col1\" class=\"data row6 col1\" >107,045</td>\n",
       "      <td id=\"T_4e350_row6_col2\" class=\"data row6 col2\" >0.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1ed02acbe08>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dermStore.stb.missing(style = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6bf5b3",
   "metadata": {},
   "source": [
    "### Version 2 -  Let's check if I'll grab only name of products without diving to list of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a6ea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DermStoreSpider_nameOnly (scrapy.Spider):\n",
    "    name = \"face_moisturizers_name_only\"\n",
    "    url = \"https://www.dermstore.com/skin-care/moisturizers/face-moisturizer.list\"\n",
    "    \n",
    "    def start_requests(self):\n",
    "        urls = ['https://www.dermstore.com/skin-care/moisturizers/face-moisturizer.list']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url = url, callback = self.parse_front)\n",
    "            \n",
    "    def parse_front(self, response):\n",
    "        product_block = response.css('div.productBlock_itemDetails_wrapper')\n",
    "        print(f\"Found {len(product_block)} products on page\")\n",
    "        product_links = product_block.xpath('./a/@href').getall()\n",
    "        print(f\"Found {len(product_links)} links on the page\")\n",
    "        for url in product_links:\n",
    "            yield response.follow(url=url, callback=self.parse_pages,\n",
    "                                 meta={'product_url':url})\n",
    "        \n",
    "        total_pages = response.xpath('//nav[@class=\"responsivePaginationPages\"]/@data-total-pages').get()\n",
    "        for page in range(2, int(total_pages) + 1):\n",
    "            next_page_url = self.url + '?pageNumber=' + str(page)\n",
    "            yield response.follow(url = next_page_url, callback=self.parse_front)\n",
    "            \n",
    "    def parse_pages(self, response):\n",
    "        product_url = response.request.meta['product_url']\n",
    "        product_name = response.xpath('//h1[contains(@class,\"productName_title\")]/text()').get()\n",
    "        item={'product_name': product_name if product_name else None, \n",
    "              'product_url': product_url if product_url else None}\n",
    "        yield item\n",
    "        \n",
    "df_dermStore_nameOnly = pd.DataFrame(columns=['product_name','product_url'])\n",
    "\n",
    "class DermStorePipeline_nameOnly:\n",
    "    \n",
    "    def process_item(self, item, spider):\n",
    "        df_dermStore_nameOnly.loc[len(df_dermStore_nameOnly)] = [item['product_name'], item['product_url']]\n",
    "        return item\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    'ITEM_PIPELINES': {'__main__.DermStorePipeline_nameOnly': 1},\n",
    "    'LOG_LEVEL': 'INFO'})\n",
    "process.crawl(DermStoreSpider_nameOnly)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a977fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dermStore_nameOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec91e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dermStore_nameOnly['product_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225aeab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
